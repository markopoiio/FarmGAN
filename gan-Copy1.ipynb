{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d64d268-16cd-4a74-992c-dc28c2704928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.random import normal\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0aeb16-1e2f-4e60-bed3-a81395f1ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = 52\n",
    "max_len = 450\n",
    "\n",
    "def get_noize():\n",
    "    return tf.random.normal(shape=(52,64,1))\n",
    "\n",
    "\n",
    "def one_hot(smi):\n",
    "    smi = \"`\" * (max_len - len(smi)) + smi\n",
    "    encoded = [[0]*allowed_chars for _ in range(max_len)]\n",
    "    for i, c in enumerate(smi):\n",
    "        encoded[i][chars[c]] = 1\n",
    "    return tf.transpose(tf.constant(encoded))\n",
    "\n",
    "    \n",
    "real_data = []\n",
    "chars = set(\"`\")\n",
    "with open(\"smiles.smi\") as f:\n",
    "    for smiles in f:\n",
    "        real_data.append(smiles[:-1])\n",
    "        chars.update(smiles[:-1])\n",
    "chars = sorted(chars)\n",
    "chars = {c: i for i,c in enumerate(chars)}\n",
    "\n",
    "real_data = [one_hot(smi) for smi in real_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e034b01d-5feb-471d-8024-1bacf829e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_bidir(units=40):\n",
    "    fw = layers.GRU(units=units, return_sequences=True, go_backwards=False)\n",
    "    bw = layers.GRU(units=units, return_sequences=True, go_backwards=True)\n",
    "    bidir = layers.Bidirectional(layer=fw, backward_layer=bw)\n",
    "    return bidir\n",
    "\n",
    "def make_generator(dict_size=52, max_smi_len=450, lstm_units=40):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(40))\n",
    "    model.add(layer_bidir(lstm_units))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(450, activation=\"softmax\"))  \n",
    "    return model\n",
    "\n",
    "def make_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(52,450)))\n",
    "    model.add(layers.Dense(40))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "cross_entropy = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13802a2e-c1bd-4a20-91ec-fb67f910d0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                    Param #     \n",
      "================================================================================\n",
      " dense (Dense)                      (52, 64, 40)                    80          \n",
      "                                                                                \n",
      " bidirectional (Bidirectional)      (52, 64, 80)                    19680       \n",
      "                                                                                \n",
      " dense_1 (Dense)                    (52, 64, 1)                     81          \n",
      "                                                                                \n",
      " flatten (Flatten)                  (52, 64)                        0           \n",
      "                                                                                \n",
      " dense_2 (Dense)                    (52, 450)                       29250       \n",
      "                                                                                \n",
      "================================================================================\n",
      "Total params: 49,091\n",
      "Trainable params: 49,091\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator()\n",
    "generator.build(input_shape=(52,64, 1))\n",
    "generator.summary(80)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8db6246-ea6a-4126-8c50-d199291fdb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                    Param #     \n",
      "================================================================================\n",
      " dense_3 (Dense)                    (None, 52, 40)                  18040       \n",
      "                                                                                \n",
      " dense_4 (Dense)                    (None, 52, 1)                   41          \n",
      "                                                                                \n",
      " flatten_1 (Flatten)                (None, 52)                      0           \n",
      "                                                                                \n",
      " dense_5 (Dense)                    (None, 1)                       53          \n",
      "                                                                                \n",
      "================================================================================\n",
      "Total params: 18,134\n",
      "Trainable params: 18,134\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator()\n",
    "discriminator.build()\n",
    "discriminator.summary(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26958be-c098-4a07-91c2-b92a5cf0c3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "epoch 1/2\n",
      "  step 604/4578; Dloss 0.018261857330799103, Gloss 4.0761275291442875\r"
     ]
    }
   ],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "\n",
    "Glosses = []\n",
    "Dlosses = []\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}\")\n",
    "    Glosses.append([])\n",
    "    Dlosses.append([])\n",
    "    \n",
    "    for i in range(len(real_data) // batch_size):\n",
    "        \n",
    "        noize = get_noize()\n",
    "        data = tf.concat([real_data[i:i+batch_size]], 0)[0]\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            fake_data = generator(noize, training=True)\n",
    "            \n",
    "            fake_data = tf.expand_dims(fake_data, 0)\n",
    "            data = tf.expand_dims(data, 0)\n",
    "            fake_output = discriminator(fake_data, training=True)\n",
    "            real_output = discriminator(data, training=True)\n",
    "            \n",
    "            gen_loss = generator_loss(fake_output)\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "            Glosses[-1].append(gen_loss)\n",
    "            Dlosses[-1].append(disc_loss)\n",
    "        \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        print(f\"  step {i}/{len(real_data)//batch_size}\", end=\"; \")\n",
    "        print(f\"Dloss {Dlosses[-1][-1]}, Gloss {Glosses[-1][-1]}\", end=\"\\r\")\n",
    "    print(f\"  step {i}/{len(real_data)//batch_size}\", end=\"; \")\n",
    "    print(f\"Dloss {Dlosses[-1][-1]}, Gloss {Glosses[-1][-1]}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6968b5b-5175-4f4f-b62a-41c681c0e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Dlosses = np.array(Dlosses)\n",
    "Glosses = np.array(Glosses)\n",
    "\n",
    "for i in range(0, num_epochs):\n",
    "    plt.plot(Dlosses[i], label=f\"Disc{i+1}\")\n",
    "    plt.plot(Glosses[i], label=f\"Gen{i+1}\")\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108f23f-7694-48f7-9364-014552bb7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_reversed = {v:k for k,v in chars.items()}\n",
    "\n",
    "def interpret_generated(generator_out):\n",
    "    smi = ''\n",
    "    out = np.array(generator_out).T\n",
    "    for i in out:\n",
    "        smi += chars_reversed[np.argmax(i)]\n",
    "    return smi\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ddeae-894b-47ed-93cb-ba8e398a8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "noize = get_noize()\n",
    "out = generator(noize, training=False)\n",
    "\n",
    "interpret_generated(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37329784-3be5-4514-9c95-5307dac22586",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(tf.expand_dims(out, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77667baf-25ce-4064-b552-0962b9417613",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = \"NCCCCC(CO)C=O\"\n",
    "real = \"`\" * (450-len(real)) + real\n",
    "real = one_hot(real)\n",
    "real = tf.expand_dims(real, 0)\n",
    "real = discriminator(real)\n",
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e22bd6-ffcf-442b-885c-65efec6f94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save(\"discriminator1.wght\")\n",
    "generator.save(\"generator1.wght\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d1eb8-914e-4494-9714-9496d6dc29e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
